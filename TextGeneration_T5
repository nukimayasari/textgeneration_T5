{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nukimayasari/text-generation-using-t-5-based-model?scriptVersionId=214635969\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:28:43.78507Z","iopub.execute_input":"2024-12-24T18:28:43.785288Z","iopub.status.idle":"2024-12-24T18:28:44.077006Z","shell.execute_reply.started":"2024-12-24T18:28:43.785267Z","shell.execute_reply":"2024-12-24T18:28:44.076186Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Text Generation using T-5 Based Model\n\nThis notebook offers a practical guide to generating text using a pre-trained T-5 based model on Kaggle. We leverage the power of the `transformers` library by Hugging Face and use `google/flan-t5-xl` to demonstrate text generation from input prompts. This notebook aims to be accessible and easy to follow, making it suitable for those new to text generation with transformer models. The code presented here was developed with the assistance of of Gemini AI.","metadata":{}},{"cell_type":"markdown","source":"\n## Setting up the Environment\n\nTo begin, we'll set up our environment by installing the required Python packages. These packages are essential for loading and using pre-trained transformer models.\n\n*   `transformers`: This library from Hugging Face provides easy access to a wide range of pre-trained language models, including T-5.\n*   `accelerate`: This library helps optimize the training and inference process, making it faster and more efficient, especially when using GPUs.\n*   `bitsandbytes`: This library allows us to use 8-bit optimizers and quantization techniques, which significantly reduce memory usage, allowing us to work with larger models even on limited hardware.\n\nAfter installing the packages, we import the necessary modules:","metadata":{}},{"cell_type":"code","source":"!pip install transformers accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:28:55.977184Z","iopub.execute_input":"2024-12-24T18:28:55.977485Z","iopub.status.idle":"2024-12-24T18:29:02.789828Z","shell.execute_reply.started":"2024-12-24T18:28:55.977464Z","shell.execute_reply":"2024-12-24T18:29:02.788744Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:37:07.799678Z","iopub.execute_input":"2024-12-24T18:37:07.80002Z","iopub.status.idle":"2024-12-24T18:37:07.803746Z","shell.execute_reply.started":"2024-12-24T18:37:07.799991Z","shell.execute_reply":"2024-12-24T18:37:07.802747Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Model and Setup Explanation\n\nLet's break down the code and the choices we've made:\n\n*   **Model ID (`model_id`):** We use `model_id = \"google/flan-t5-xl\"`. This specifies the pre-trained model we're loading from the Hugging Face Model Hub. Flan-T5 is based on the T5 architecture, which is excellent for various text-to-text tasks, including:\n    *   Text Generation\n    *   Translation\n    *   Summarization\n    *   Question Answering\n\n*   **`AutoModelForSeq2SeqLM`:** Because Flan-T5 is a *sequence-to-sequence* model, we use `AutoModelForSeq2SeqLM`. This is different from *causal* language models (like GPT), which are designed for predicting the next word in a sequence. Sequence-to-sequence models are more general-purpose and can handle tasks where the input and output have different lengths or structures.\n\n*   **`input_ids=input_ids.input_ids`:** When using the `generate` function with sequence-to-sequence models, the input IDs (the numerical representation of your text) must be passed in this specific way. It's how the model receives the encoded input.\n\n### Why Flan-T5-XL?\n\nWe chose Flan-T5-XL for several reasons:\n\n*   **Publicly Available:** It's readily accessible; you don't need to request special access.\n*   **Strong Performance:** Flan-T5 models are known for their state-of-the-art performance on a wide range of NLP benchmarks.\n*   **Suitable for Text Generation:** It's particularly effective at generating coherent and relevant text based on prompts.\n\n### How to Use This in Your Notebook\n\nHere's a quick guide to using this code in your Kaggle Notebook:\n\n1.  **Create/Open Notebook:** Create a new Kaggle Notebook or continue working in an existing one.\n2.  **Paste and Run:** Copy and paste the code into a code cell and run it.\n3.  **Experiment with Prompts:** Change the `prompt` variable to any text you want the model to generate from.\n4.  **Adjust `max_new_tokens`:** This parameter in the `model.generate()` function controls the maximum length of the generated text. Increase it for longer outputs, decrease it for shorter ones.","metadata":{}},{"cell_type":"code","source":"model_id = \"google/flan-t5-xl\"  # Use a publicly available model\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\nprompt = \"Describe a cute dog.\" #we can experiment with this section\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(input_ids=input_ids.input_ids, max_new_tokens=200)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:44:42.073886Z","iopub.execute_input":"2024-12-24T18:44:42.074228Z","iopub.status.idle":"2024-12-24T18:44:48.570638Z","shell.execute_reply.started":"2024-12-24T18:44:42.0742Z","shell.execute_reply":"2024-12-24T18:44:48.569688Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9844b27e1605492dab8f8d4107716637"}},"metadata":{}},{"name":"stdout","text":"a chihuahua\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Generating Text with Multiple Prompts (Batching)\n\nNow, let's explore how to generate text from *multiple* prompts efficiently. Instead of processing each prompt individually, we can use *batching*, which significantly speeds up the process, especially when dealing with many inputs.","metadata":{}},{"cell_type":"code","source":"prompts2 = [\n    \"Write a short story about a cat in love.\", #we can experiment on this section\n    \"Write a poem about dog and bone.\", #we can experiment on this section\n    \"Translate 'Hello, how are you?' to Italian.\" #we can experiment on this section\n]\n\ninputs2 = tokenizer(prompts2, return_tensors=\"pt\", padding=True).to(model.device) #padding is important here\n\noutputs2 = model.generate(input_ids=inputs2.input_ids, max_new_tokens=200)\n\ngenerated_texts2 = tokenizer.batch_decode(outputs2, skip_special_tokens=True)\n\nfor i, text in enumerate(generated_texts2):\n    print(f\"Prompt {i+1}:\\n{prompts2[i]}\\nGenerated Text:\\n{text}\\n---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:52:30.693313Z","iopub.execute_input":"2024-12-24T18:52:30.693658Z","iopub.status.idle":"2024-12-24T18:52:56.68589Z","shell.execute_reply.started":"2024-12-24T18:52:30.693629Z","shell.execute_reply":"2024-12-24T18:52:56.685085Z"}},"outputs":[{"name":"stdout","text":"Prompt 1:\nWrite a short story about a cat in love.\nGenerated Text:\nThe cat was in love with the dog. The cat was in love with the dog. The cat was in love with the dog.\n---\nPrompt 2:\nWrite a poem about dog and bone.\nGenerated Text:\ni have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i have a bone i\n---\nPrompt 3:\nTranslate 'Hello, how are you?' to Italian.\nGenerated Text:\nDove si è?\n---\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### Generating Text with Multiple Prompts (Looping)\n\nWhile batching (as shown in the previous section) is generally the most efficient method for handling multiple prompts, sometimes we might want to process prompts individually, especially if we have a very small number of inputs or need to perform some processing between each generation. This section demonstrates how to achieve this using a loop.","metadata":{}},{"cell_type":"code","source":"prompts3 = [\n    \"Write a short story about a broken hearted cat.\", #we can experiment on this section\n    \"Write a poem about the beauty of hot chocolate.\", #we can experiment on this section\n]\n\nfor prompt in prompts3:\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(input_ids=input_ids.input_ids, max_new_tokens=200)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Prompt:\\n{prompt}\\nGenerated Text:\\n{generated_text}\\n---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:55:02.22371Z","iopub.execute_input":"2024-12-24T18:55:02.224007Z","iopub.status.idle":"2024-12-24T18:55:17.738662Z","shell.execute_reply.started":"2024-12-24T18:55:02.223986Z","shell.execute_reply":"2024-12-24T18:55:17.737721Z"}},"outputs":[{"name":"stdout","text":"Prompt:\nWrite a short story about a broken hearted cat.\nGenerated Text:\nThe cat was very sad. She was very sad because she had lost her favorite cat. She was very sad because she had lost her favorite cat. She was very sad because she had lost her favorite cat.\n---\nPrompt:\nWrite a poem about the beauty of hot chocolate.\nGenerated Text:\ni love hot chocolate i love it so much i can eat it all day i love it so much i can drink it all day i love it so much i can eat it all day i love it so much i can drink it all day i love it so much i can eat it all day i love it so much i can drink it all day i love it so much i can eat it all day i love it so much i can drink it all day i love it so much i can eat it all day i love it so much i can drink it all day i love it so much i can eat it all day i love it so much i can drink it all day i love it so much i can eat it all day i love it so much i can drink it all day i love it so much\n---\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Conclusion and Takeaways\n\nIn this notebook, we've explored the exciting field of text generation using the powerful `google/flan-t5-xl` model. We've seen how to:\n\n*   Load a pre-trained T-5 model using the `transformers` library.\n*   Generate text from single prompts.\n*   Generate text from multiple prompts using both batching (for efficiency) and looping (for individual processing).\n\nAs a beginner in the world of NLP and large language models, I wanted to share a clear and accessible guide to this topic. My goal is that this notebook serves as a helpful resource for other beginners who are also just starting to learn about text generation and transformer models. I hope you found it useful!","metadata":{}}]}